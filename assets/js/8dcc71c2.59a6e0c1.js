"use strict";(self.webpackChunkcf_infra_docs=self.webpackChunkcf_infra_docs||[]).push([[65394],{84833:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});var t=s(29643),i=s(74848),o=s(28453);const a={},r="Updating our default docker images",c={authorsImageUrls:[]},l=[];function d(e){const n={a:"a",code:"code",em:"em",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"TL;DR: We have made some updates to our Docker images and build time GLIBC selection."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"We've updated our default docker images to be based on alma9"}),"\n",(0,i.jsxs)(n.li,{children:["It is now easier to override ",(0,i.jsx)(n.code,{children:"c_stdlib_version"})," (especially for CUDA-enabled feedstocks), though our baseline of 2.17 hasn't changed."]}),"\n",(0,i.jsxs)(n.li,{children:["Where necessary, you can more easily switch images by setting ",(0,i.jsx)(n.code,{children:"os_version: ..."})," (see below)."]}),"\n",(0,i.jsx)(n.li,{children:"We've consolidated our image names to follow a consistent pattern:"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"linux-anvil-{x86_64,aarch64,ppc64le}:{cos7,alma8,alma9}\n"})}),"\n",(0,i.jsxs)(n.p,{children:["In general, it won't be necessary in the vast majority of cases to override the\ndocker-image, but if you need to do so, you can add the following to ",(0,i.jsx)(n.code,{children:"conda-forge.yml"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"os_version:             # just to demo different values;\n  linux_64: cos7        # whenever possible, please use\n  linux_aarch64: alma8  # homogeneous distro versions\n  linux_ppc64le: alma9  # across platforms\n"})}),"\n",(0,i.jsx)(n.p,{children:"Linux builds in conda-forge run on infrastructure derived from RHEL and its clones\n-- previously CentOS, now AlmaLinux. Primarily we need this for four different\ninterrelated but distinct pieces:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"the docker images (containing the OS which will execute our builds)"}),"\n",(0,i.jsxs)(n.li,{children:["the sysroot (mainly the C standard library, ",(0,i.jsx)(n.code,{children:"glibc"}),")"]}),"\n",(0,i.jsx)(n.li,{children:"the CDTs (pieces from the distribution we cannot package ourselves)"}),"\n",(0,i.jsxs)(n.li,{children:["feedstock usage of ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["A first key observation is that the glibc appears twice -- once explicitly in the\nsysroot we package (and compile against!), and once implicitly in the image that\nour CI runs on. This setup is essential to provide highly compatible packages by\ndefault (by compiling against a cos7 baseline), while avoiding constant hassles\nfor feedstocks where ",(0,i.jsx)(n.em,{children:"any"})," of the build/host/run dependencies requires a newer\nglibc than the baseline."]}),"\n",(0,i.jsxs)(n.p,{children:["This is because, for packages requiring a newer ",(0,i.jsx)(n.code,{children:"c_stdlib_version"})," (and thus compiling\nagainst a newer sysroot through the ",(0,i.jsx)(n.code,{children:'{{ stdlib("c") }}'})," infrastructure), will inherit\na runtime-requirement of ",(0,i.jsx)(n.code,{children:"__glibc >=c_stdlib_version"}),", which would be unsatisfiable on\ndocker-images with a too-old glibc present at runtime."]}),"\n",(0,i.jsxs)(n.p,{children:["We've already had this setup since 2021 (when our glibc baseline was 2.12 from cos6,\nyet we already used cos7 images), but after increasing the glibc baseline to 2.17, our\nimages had lost their lead again. This is mostly related due to the third component from\nabove, the CDTs (core dependency trees). These represent packages from the distribution\nitself that are hard or impossible for us to provide, yet need a systematic way to\ninteract with. You can read more about ",(0,i.jsx)(n.em,{children:"why we want to avoid them as much as possible"}),"\n",(0,i.jsx)(n.a,{href:"https://conda-forge.org/docs/maintainer/knowledge_base/#why-are-cdts-bad",children:"here"}),"."]}),"\n",(0,i.jsx)(n.p,{children:"Due to the end of CentOS-as-we-knew it, we already had to rewrite a lot of the logic\nthere in any case to switch to Alma, which we took as an opportunity to pare down the\nset of CDTs we provide going forward. In a large majority of cases, we have regular\nconda packages for some things that only used to be available as CDTs."}),"\n",(0,i.jsxs)(n.p,{children:["CDTs and packages in ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"})," are closely related; in many ways it can\nbe considered a similar compilation-vs.-runtime split as is the case with our sysroot\n(that we compile against) vs. the glibc in the image at runtime. The split here being\nthat CDTs are what we use to compile against a given distro package, and ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"}),"\nare how we tell the infrastructure to install them into the image, if they're also\nnecessary at runtime (which is not always the case)."]}),"\n",(0,i.jsxs)(n.p,{children:["In other words, using our own packages generally allows feedstocks to avoid ",(0,i.jsx)(n.em,{children:"both"})," use\nof CDTs and ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"}),". You can check out the CDTs we removed\n",(0,i.jsx)(n.a,{href:"https://github.com/conda-forge/cdt-builds/issues/66#issuecomment-1833417828",children:"here"}),"\nand how ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"})," translate from CentOS to Alma (resp. our own packages)\n",(0,i.jsx)(n.a,{href:"https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/6283#issuecomment-2440281086",children:"here"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The change of the image might mean that CDTs we have not repackaged for Alma do not\nmatch what's actually in the image anymore, or -- in rare cases -- that a package name\nunder ",(0,i.jsx)(n.code,{children:"yum_requirements.txt"})," needs to be updated. Please let us know if you run into\nproblems there (after checking out the two links above how to transition a given package)."]}),"\n",(0,i.jsxs)(n.p,{children:["Finally, there is one rare case where we explicitly ask feedstock authors to opt out\nof the newest images: for any feedstocks doing binary repackaging on linux (i.e. not\ncompiling the package from source), please ensure that your image version (as specified\nin ",(0,i.jsx)(n.code,{children:"conda-forge.yml"}),", see above) matches the ",(0,i.jsx)(n.code,{children:"c_stdlib_version"})," that you are using.\nBy default this is 2.17, which means you'd have to do"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"os_version:\n  linux_64: cos7\n  linux_aarch64: cos7\n  linux_ppc64le: cos7\n"})}),"\n",(0,i.jsxs)(n.p,{children:["If you require a ",(0,i.jsx)(n.code,{children:"c_stdlib_version"})," of 2.28 for a given platform, then set ",(0,i.jsx)(n.code,{children:"alma8"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>r});var t=s(96540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},29643:e=>{e.exports=JSON.parse('{"permalink":"/news/2024/11/22/new-images","source":"@site/news/2024-11-22-new-images.md","title":"Updating our default docker images","description":"TL;DR: We have made some updates to our Docker images and build time GLIBC selection.","date":"2024-11-22T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Migration to Unique Feedstock Tokens per Provider","permalink":"/news/2024/11/08/unique-feedstock-token-per-provider-migration"}}')}}]);