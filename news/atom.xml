<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://conda-forge.org/news/</id>
    <title>conda-forge | community-driven packaging for conda Blog</title>
    <updated>2025-10-15T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://conda-forge.org/news/"/>
    <subtitle>conda-forge | community-driven packaging for conda Blog</subtitle>
    <icon>https://conda-forge.org/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[conda-forge Discourse forum is now read-only]]></title>
        <id>https://conda-forge.org/news/2025/10/15/conda-forge-discourse-read-only/</id>
        <link href="https://conda-forge.org/news/2025/10/15/conda-forge-discourse-read-only/"/>
        <updated>2025-10-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We have made the conda-forge Discourse forum read-only. Please use the conda-forge Zulip chat instead.]]></summary>
        <content type="html"><![CDATA[<p>We have made the <a href="https://conda.discourse.group/c/pkg-building/conda-forge/25" target="_blank" rel="noopener noreferrer" class="">conda-forge Discourse forum</a> read-only. Please use the conda-forge <a href="https://conda-forge.zulipchat.com/" target="_blank" rel="noopener noreferrer" class="">Zulip chat</a> instead.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dropping Python 3.9 support in conda-forge]]></title>
        <id>https://conda-forge.org/news/2025/08/18/python-3-9/</id>
        <link href="https://conda-forge.org/news/2025/08/18/python-3-9/"/>
        <updated>2025-08-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[With Python 3.9 reaching end-of-life in Oct 2025 and Python]]></summary>
        <content type="html"><![CDATA[<p>With Python 3.9 reaching end-of-life in Oct 2025 and Python
3.14 being released the same month, we have decided to drop
3.9 from our default build matrix. This will be reflected
in your feedstock configuration on the next rerender.
The decision to drop support 1.5 months before its EOL is
to avoid the strain on conda-forge CI while we add support
for 3.14.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Accelerate support for macOS 13.3+]]></title>
        <id>https://conda-forge.org/news/2025/07/31/new-accelerate-macos/</id>
        <link href="https://conda-forge.org/news/2025/07/31/new-accelerate-macos/"/>
        <updated>2025-07-31T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[conda-forge by default uses OpenBLAS as its BLAS and LAPACK]]></summary>
        <content type="html"><![CDATA[<p>conda-forge by default uses <code>OpenBLAS</code> as its BLAS and LAPACK
provider on macOS as it was updated regularly and is the least
buggiest performant BLAS/LAPACK implementation.</p>
<p>macOS 13.3 updated the Accelerate framework after a long time with
improved support for LAPACK APIs and has fixes for long-time known
bugs in the older Accelerate's BLAS and LAPACK APIs.
conda-forge has added support for this
new Accelerate framework by using a shim library to expose its
functionality to most conda-forge packages including <code>numpy</code>,
<code>scipy</code> and <code>pytorch</code>.</p>
<p>You can use it by doing</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">conda install libblas=*=*_newaccelerate</span><br></span></code></pre></div></div>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moving to GCC 14 and Clang 19 as default compiler versions]]></title>
        <id>https://conda-forge.org/news/2025/07/01/moving-to-gcc-14-clang-19/</id>
        <link href="https://conda-forge.org/news/2025/07/01/moving-to-gcc-14-clang-19/"/>
        <updated>2025-07-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[As part of our regular toolchain updates, we're planning to update the default]]></summary>
        <content type="html"><![CDATA[<p>As part of our regular toolchain updates, we're planning to update the default
versions of GCC (for linux) to v14 and of Clang (for OSX) to v19, in one week.</p>
<p>For more details, see <a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/pull/7421" target="_blank" rel="noopener noreferrer" class="">https://github.com/conda-forge/conda-forge-pinning-feedstock/pull/7421</a>.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moving to Visual Studio 2022 as default windows compiler]]></title>
        <id>https://conda-forge.org/news/2025/06/11/moving-to-vs2022/</id>
        <link href="https://conda-forge.org/news/2025/06/11/moving-to-vs2022/"/>
        <updated>2025-06-11T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Microsoft's Visual Studio (VS) 2019 compiler has reached its]]></summary>
        <content type="html"><![CDATA[<p>Microsoft's Visual Studio (VS) 2019 compiler has reached its
<a href="https://learn.microsoft.com/en-us/lifecycle/products/visual-studio-2019" target="_blank" rel="noopener noreferrer" class="">end of life</a>
over a year ago. In the meantime, several projects have moved on and
fail to compile with VS2019.</p>
<p>We are planning to update our default compilers on windows to the (fully compatible)
successor VS2022 in one week from now.</p>
<p>This will not affect you as a general user of conda-forge packages on windows;
the only potential impact is that if you are compiling locally with VS2019 against
artefacts produced by conda-forge, you might be required to upgrade.</p>
<p>For more details see <a href="https://github.com/conda-forge/conda-forge.github.io/issues/2138" target="_blank" rel="noopener noreferrer" class="">https://github.com/conda-forge/conda-forge.github.io/issues/2138</a>.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dropping CUDA 11.8 as a default CUDA version]]></title>
        <id>https://conda-forge.org/news/2025/05/29/cuda-118/</id>
        <link href="https://conda-forge.org/news/2025/05/29/cuda-118/"/>
        <updated>2025-05-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[CUDA 11.8 is the last holdover from the old days before conda-forge]]></summary>
        <content type="html"><![CDATA[<p>CUDA 11.8 is the last holdover from the old days before conda-forge
<a href="https://github.com/conda-forge/conda-forge.github.io/issues/1963" target="_blank" rel="noopener noreferrer" class="">switched</a>
to the new and shiny CUDA 12+ infrastructure, where the CUDA toolchain
is provided as native conda-packages, rather than a blob in an image.</p>
<p>For CUDA-enabled feedstocks, we've been building both 11.8 and 12.6 by default
for a while now, but many feedstocks (notably pytorch, tensorflow, onnx, jax etc.)
have dropped CUDA 11.8 for many months already.</p>
<p>Due to various constraints (details below), we are dropping CUDA 11.8 as a default
version in our global pinning on June 5th. It will still be possible to opt into
building CUDA 11.8 on a per-feedstock basis where this is necessary or beneficial.</p>
<p>The above-mentioned contraints are mainly:</p>
<ul>
<li class="">it <a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/6967" target="_blank" rel="noopener noreferrer" class="">complicates our pinning</a> due to needing to switch images and compilers with 11.8.</li>
<li class="">it keeps us from <a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/pull/7005" target="_blank" rel="noopener noreferrer" class="">migrating</a>
to newer CUDA 12.x versions necessary to support new architectures.</li>
<li class="">it's <a href="https://github.com/conda-forge/conda-forge.github.io/issues/2138#issuecomment-2916743741" target="_blank" rel="noopener noreferrer" class="">not compatible with VS2022</a>, which is due to become the default toolchain on windows
in conda-forge soon (the previous VS2019 has reached end-of-life more than a year ago).</li>
<li class="">it complicates our infrastructure in several places, due to the big differences between the
before/after of the new CUDA architecture.</li>
</ul>
<p>After we have removed CUDA 11.8 from the pinning, any feedstock still building that version
will drop the respective CI jobs upon rerendering. For feedstocks wanting to keep building
CUDA 11.8 a bit longer, we have provided a custom migrator.</p>
<p>The way to make use of this is to copy
<a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/blob/main/recipe/migrations/cuda118.yaml" target="_blank" rel="noopener noreferrer" class=""><code>cuda118.yaml</code></a>
from the global pinning into <code>.ci_support/migrations</code> on your feedstock.
If the <code>migrations</code> subfolder doesn't exist, please create it. Once that's committed
(and there are no skips in the recipe for CUDA 11.8), rerendering the feedstock
will reinstate the builds for CUDA 11.8. If you have trouble with that, please open
a thread on <a href="https://conda-forge.zulipchat.com/" target="_blank" rel="noopener noreferrer" class="">https://conda-forge.zulipchat.com/</a>.</p>
<p>Finally, please let us
know in the <a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/7404" target="_blank" rel="noopener noreferrer" class="">issue</a>
if your feedstock still needs to support CUDA 11.8 and why (later down the line we'll want to
drop support also in conda-forge-ci-setup, and knowing what feedstocks - if any - still need
CUDA 11.8 will help guide the decision on timing).</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Upcoming closure of NumPy 2.0 migration]]></title>
        <id>https://conda-forge.org/news/2025/05/28/numpy-2-migration-closure/</id>
        <link href="https://conda-forge.org/news/2025/05/28/numpy-2-migration-closure/"/>
        <updated>2025-05-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[NumPy 2.0 was a big change (the first major version in 15 years). For more than a year, we]]></summary>
        <content type="html"><![CDATA[<p>NumPy 2.0 was a big change (the first major version in 15 years). For more than a year, we
have been migrating feedstocks from NumPy 1.x to NumPy 2.x, and while not every affected
feedstock has been done, we are planning to conclude the migration in one week.
Note that NumPy 2 support is required for feedstocks that intend to support Python 3.13
and above.</p>
<p>For feedstocks that are not compatible with v2.x yet, this means you will have to add</p>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token key atrule" style="color:#00a4db">numpy</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  </span><span class="token punctuation" style="color:#393A34">-</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1.26</span><span class="token plain">  </span><span class="token comment" style="color:#999988;font-style:italic"># or 1.25</span><br></span></code></pre></div></div>
<p>to your <code>recipe/conda_build_config.yaml</code>, and then rerender. Pins below 1.25 are not possible
if your feedstock supports Python 3.12, as NumPy 1.25 was the first version with support for
that Python version (and it will not be possible going forward to pin different NumPy versions
for different Python versions).</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Governance document moved to conda-forge/governance]]></title>
        <id>https://conda-forge.org/news/2025/05/08/governance-moved/</id>
        <link href="https://conda-forge.org/news/2025/05/08/governance-moved/"/>
        <updated>2025-05-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Following a change in our governance, this document lives now in conda-forge/governance, along with the CSV files that list the core and emeritus members.]]></summary>
        <content type="html"><![CDATA[<p>Following a <a href="https://github.com/conda-forge/conda-forge.github.io/pull/2501" target="_blank" rel="noopener noreferrer" class="">change</a> in our governance, this document lives now in <a href="https://github.com/conda-forge/governance" target="_blank" rel="noopener noreferrer" class=""><code>conda-forge/governance</code></a>, along with the CSV files that list the <code>core</code> and <code>emeritus</code> members.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating our Ubuntu base for Miniforge Docker images (20.04 ‚Üí 24.04)]]></title>
        <id>https://conda-forge.org/news/2025/04/17/new-ubuntu-base-for-miniforge-docker-images/</id>
        <link href="https://conda-forge.org/news/2025/04/17/new-ubuntu-base-for-miniforge-docker-images/"/>
        <updated>2025-04-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The base image for our Ubuntu Dockerfiles has been upgraded from Ubuntu 20.04]]></summary>
        <content type="html"><![CDATA[<p>The base image for our Ubuntu Dockerfiles has been upgraded from Ubuntu 20.04
(focal) to Ubuntu 24.04 (noble) in
<a href="https://github.com/conda-forge/miniforge-images/pull/145" target="_blank" rel="noopener noreferrer" class="">PR #145</a>.
This change ensures continued support and access to newer packages and system
libraries.</p>
<p>Downstream users building on top of the Ubuntu variant of our containers should
verify compatibility with the updated environment.</p>
<p>Thanks to @rpanai for the contribution and to the reviewers for their input.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security Incident with Package Uploads (CVE-2025-31484)]]></title>
        <id>https://conda-forge.org/news/2025/04/02/Security-Incident-with-Package-Uploads/</id>
        <link href="https://conda-forge.org/news/2025/04/02/Security-Incident-with-Package-Uploads/"/>
        <updated>2025-04-02T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Yesterday, conda-forge was notified of a security incident reporting that the anaconda.org upload token]]></summary>
        <content type="html"><![CDATA[<p>Yesterday, <code>conda-forge</code> was notified of a security incident reporting that the <code>anaconda.org</code> upload token
for the <code>conda-forge</code> channel had been accidentally leaked between on or about 2025-02-10 to 2025-04-01. Our
investigation resulted in the temporary artifact upload shutdown you observed yesterday (2025-04-01). The results
of our analysis show that, as best as can reasonably be determined, the token was not used by any 3rd party to
upload malicious artifacts.</p>
<p>More details in the <a class="" href="https://conda-forge.org/blog/2025/04/02/security-incident-with-package-uploads/">corresponding blog post</a>.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Updating our default docker images]]></title>
        <id>https://conda-forge.org/news/2024/11/22/new-images/</id>
        <link href="https://conda-forge.org/news/2024/11/22/new-images/"/>
        <updated>2024-11-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR: We have made some updates to our Docker images and build time GLIBC selection.]]></summary>
        <content type="html"><![CDATA[<p>TL;DR: We have made some updates to our Docker images and build time GLIBC selection.</p>
<ol>
<li class="">We've updated our default docker images to be based on alma9</li>
<li class="">It is now easier to override <code>c_stdlib_version</code> (especially for CUDA-enabled feedstocks), though our baseline of 2.17 hasn't changed.</li>
<li class="">Where necessary, you can more easily switch images by setting <code>os_version: ...</code> (see below).</li>
<li class="">We've consolidated our image names to follow a consistent pattern:</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">linux-anvil-{x86_64,aarch64,ppc64le}:{cos7,alma8,alma9}</span><br></span></code></pre></div></div>
<p>In general, it won't be necessary in the vast majority of cases to override the
docker-image, but if you need to do so, you can add the following to <code>conda-forge.yml</code></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">os_version:             # just to demo different values;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_64: cos7        # whenever possible, please use</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_aarch64: alma8  # homogeneous distro versions</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_ppc64le: alma9  # across platforms</span><br></span></code></pre></div></div>
<p>Linux builds in conda-forge run on infrastructure derived from RHEL and its clones
-- previously CentOS, now AlmaLinux. Primarily we need this for four different
interrelated but distinct pieces:</p>
<ul>
<li class="">the docker images (containing the OS which will execute our builds)</li>
<li class="">the sysroot (mainly the C standard library, <code>glibc</code>)</li>
<li class="">the CDTs (pieces from the distribution we cannot package ourselves)</li>
<li class="">feedstock usage of <code>yum_requirements.txt</code></li>
</ul>
<p>A first key observation is that the glibc appears twice -- once explicitly in the
sysroot we package (and compile against!), and once implicitly in the image that
our CI runs on. This setup is essential to provide highly compatible packages by
default (by compiling against a cos7 baseline), while avoiding constant hassles
for feedstocks where <em>any</em> of the build/host/run dependencies requires a newer
glibc than the baseline.</p>
<p>This is because, for packages requiring a newer <code>c_stdlib_version</code> (and thus compiling
against a newer sysroot through the <code>{{ stdlib("c") }}</code> infrastructure), will inherit
a runtime-requirement of <code>__glibc &gt;=c_stdlib_version</code>, which would be unsatisfiable on
docker-images with a too-old glibc present at runtime.</p>
<p>We've already had this setup since 2021 (when our glibc baseline was 2.12 from cos6,
yet we already used cos7 images), but after increasing the glibc baseline to 2.17, our
images had lost their lead again. This is mostly related due to the third component from
above, the CDTs (core dependency trees). These represent packages from the distribution
itself that are hard or impossible for us to provide, yet need a systematic way to
interact with. You can read more about <em>why we want to avoid them as much as possible</em>
<a href="https://conda-forge.org/docs/maintainer/knowledge_base/#why-are-cdts-bad" target="_blank" rel="noopener noreferrer" class="">here</a>.</p>
<p>Due to the end of CentOS-as-we-knew it, we already had to rewrite a lot of the logic
there in any case to switch to Alma, which we took as an opportunity to pare down the
set of CDTs we provide going forward. In a large majority of cases, we have regular
conda packages for some things that only used to be available as CDTs.</p>
<p>CDTs and packages in <code>yum_requirements.txt</code> are closely related; in many ways it can
be considered a similar compilation-vs.-runtime split as is the case with our sysroot
(that we compile against) vs. the glibc in the image at runtime. The split here being
that CDTs are what we use to compile against a given distro package, and <code>yum_requirements.txt</code>
are how we tell the infrastructure to install them into the image, if they're also
necessary at runtime (which is not always the case).</p>
<p>In other words, using our own packages generally allows feedstocks to avoid <em>both</em> use
of CDTs and <code>yum_requirements.txt</code>. You can check out the CDTs we removed
<a href="https://github.com/conda-forge/cdt-builds/issues/66#issuecomment-1833417828" target="_blank" rel="noopener noreferrer" class="">here</a>
and how <code>yum_requirements.txt</code> translate from CentOS to Alma (resp. our own packages)
<a href="https://github.com/conda-forge/conda-forge-pinning-feedstock/issues/6283#issuecomment-2440281086" target="_blank" rel="noopener noreferrer" class="">here</a>.</p>
<p>The change of the image might mean that CDTs we have not repackaged for Alma do not
match what's actually in the image anymore, or -- in rare cases -- that a package name
under <code>yum_requirements.txt</code> needs to be updated. Please let us know if you run into
problems there (after checking out the two links above how to transition a given package).</p>
<p>Finally, there is one rare case where we explicitly ask feedstock authors to opt out
of the newest images: for any feedstocks doing binary repackaging on linux (i.e. not
compiling the package from source), please ensure that your image version (as specified
in <code>conda-forge.yml</code>, see above) matches the <code>c_stdlib_version</code> that you are using.
By default this is 2.17, which means you'd have to do</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">os_version:</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_64: cos7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_aarch64: cos7</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  linux_ppc64le: cos7</span><br></span></code></pre></div></div>
<p>If you require a <code>c_stdlib_version</code> of 2.28 for a given platform, then set <code>alma8</code>.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Migration to Unique Feedstock Tokens per Provider]]></title>
        <id>https://conda-forge.org/news/2024/11/08/unique-feedstock-token-per-provider-migration/</id>
        <link href="https://conda-forge.org/news/2024/11/08/unique-feedstock-token-per-provider-migration/"/>
        <updated>2024-11-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We will be slowly migrating conda-forge to use unique feedstock tokens per provider. The feedstock token is used to allow maintainers to copy packages from our staging area to the main conda-forge channel. This change will improve our security posture and help us limit the impact of any leaked tokens. During this migration we will also be using newly implemented feedstock token expiration times to avoid race conditions between token changes and running builds.]]></summary>
        <content type="html"><![CDATA[<p>We will be slowly migrating <code>conda-forge</code> to use unique feedstock tokens per provider. The feedstock token is used to allow maintainers to copy packages from our staging area to the main <code>conda-forge</code> channel. This change will improve our security posture and help us limit the impact of any leaked tokens. During this migration we will also be using newly implemented feedstock token expiration times to avoid race conditions between token changes and running builds.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[New time available for conda-forge core meetings]]></title>
        <id>https://conda-forge.org/news/2024/11/07/new-time-core-meetings/</id>
        <link href="https://conda-forge.org/news/2024/11/07/new-time-core-meetings/"/>
        <updated>2024-11-07T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The core team has decided to change the time when core meetings happen to accommodate more attendees across different timezones. It will still happen every two Wednesdays, but starting next Wednesday, November 13th 2024, it will alternate between 1700 UTC and 1400 UTC.]]></summary>
        <content type="html"><![CDATA[<p>The core team has decided to change the time when core meetings happen to accommodate more attendees across different timezones. It will still happen every two Wednesdays, but starting next Wednesday, November 13th 2024, it will alternate between 17:00-18:00 UTC and 14:00-15:00 UTC.</p>
<p>For clarity, these are the next dates:</p>
<ul>
<li class="">November 13th, 2024 at 17:00 UTC</li>
<li class="">November 27th, 2024 at 14:00 UTC</li>
<li class="">December 11th, 2024 at 17:00 UTC</li>
<li class=""><del>December 25th, 2024 at 14:00 UTC</del></li>
<li class="">January 8th, 2025, at 17:00 UTC</li>
<li class="">... and so on.</li>
</ul>
<p>A new calendar is now available in the <a class="" href="https://conda-forge.org/community/meetings/">Community &gt; Meetings</a> section to help find the dates.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moving to Zulip]]></title>
        <id>https://conda-forge.org/news/2024/11/04/moving-to-zulip/</id>
        <link href="https://conda-forge.org/news/2024/11/04/moving-to-zulip/"/>
        <updated>2024-11-04T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two weeks ago we called a vote on CFEP-23 to decide whether we move our Element/Matrix chat rooms to Zulip.]]></summary>
        <content type="html"><![CDATA[<p>Two weeks ago we called a <a href="https://github.com/conda-forge/cfep/pull/54" target="_blank" rel="noopener noreferrer" class="">vote</a> on <a href="https://github.com/conda-forge/cfep/blob/main/cfep-23.md" target="_blank" rel="noopener noreferrer" class="">CFEP-23</a> to decide whether we move our <a href="https://matrix.to/#/#conda-forge:matrix.org" target="_blank" rel="noopener noreferrer" class="">Element/Matrix chat rooms</a> to Zulip.</p>
<p>This vote has passed and now we are opening the doors to our Zulip instance: <a href="https://conda-forge.zulipchat.com/" target="_blank" rel="noopener noreferrer" class="">https://conda-forge.zulipchat.com/</a>. Please sign up to stay in touch!</p>
<p>As per <a href="https://github.com/conda-forge/cfep/blob/main/cfep-23.md" target="_blank" rel="noopener noreferrer" class="">CFEP-23</a> (read it for more details), this means that we will stop using our <a href="https://matrix.to/#/#conda-forge:matrix.org" target="_blank" rel="noopener noreferrer" class="">Element chat rooms</a>. Instead all chat activity will continue in Zulip.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Changes to the Feedstock Output Validation Procedure]]></title>
        <id>https://conda-forge.org/news/2024/09/08/output-validation-changes/</id>
        <link href="https://conda-forge.org/news/2024/09/08/output-validation-changes/"/>
        <updated>2024-09-08T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[We have changed our feedstock output validation procedure. Instead of automatically]]></summary>
        <content type="html"><![CDATA[<p>We have changed our feedstock output validation procedure. Instead of automatically
adding new packages to existing feedstocks (e.g., when a new output is added), we will now
require that maintainers submit a PR to add the new output via our
<a href="https://github.com/conda-forge/admin-requests?tab=readme-ov-file#add-a-package-output-to-a-feedstock" target="_blank" rel="noopener noreferrer" class="">admin-requests repository</a>.
New feedstocks created via <code>staged-recipes</code> will have their outputs added automatically. If you maintain a feedstock
that regularly adds new outputs (e.g., <code>llvmdev</code> has <code>libllvm18</code>, <code>libllvm19</code>, etc.), you can use the <code>admin-requsts</code> repo
to add a glob pattern that matches the pattern of your new outputs. We test these patterns aginst new outputs from the feedstock
using the Python <code>fnmatch</code> module. Outputs from the feedstock that match a glob pattern will added automatically. Further
details on package output validation can be found in our <a href="https://conda-forge.org/docs/maintainer/infrastructure/#output-validation-and-feedstock-tokens" target="_blank" rel="noopener noreferrer" class="">documentation</a>.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing build in favor of python-build]]></title>
        <id>https://conda-forge.org/news/2024/08/21/remove-build/</id>
        <link href="https://conda-forge.org/news/2024/08/21/remove-build/"/>
        <updated>2024-08-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[For nearly two years now, we have favored the use python-build as the package name for the PyPA build]]></summary>
        <content type="html"><![CDATA[<p>For nearly two years now, we have favored the use <code>python-build</code> as the package name for the PyPA <a href="https://github.com/pypa/build" target="_blank" rel="noopener noreferrer" class=""><code>build</code></a>
project over <code>build</code>. In fact, the <code>build</code> feedstock on <code>conda-forge</code> is archived and has not had its version
updated. To complete this transition, we are going to mark all existing <code>build</code> packages as broken, provide
a more detailed linter hint, and turn off the migration infrastructure we've been using to help move feedstocks.
Any questions or comments can be directed to the <a href="https://github.com/conda-forge/conda-forge.github.io/issues/2269" target="_blank" rel="noopener noreferrer" class="">GitHub issue</a> on this work.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing wheel and setuptools as Dependencies for pip]]></title>
        <id>https://conda-forge.org/news/2024/08/21/sunsetting-pip-deps/</id>
        <link href="https://conda-forge.org/news/2024/08/21/sunsetting-pip-deps/"/>
        <updated>2024-08-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Starting with Python 3.13, pip will no longer have setuptools and wheel as dependencies]]></summary>
        <content type="html"><![CDATA[<p>Starting with Python 3.13, <code>pip</code> will no longer have <code>setuptools</code> and <code>wheel</code> as dependencies
by default. While this default made sense at the time <code>conda-forge</code> was started (over 8 years ago!),
with the advent of Python packaging build backends (e.g., <code>flit</code>, <code>poetry</code>, <code>hatchling</code>, etc.),
this default is no longer correct. Instead, you will need to specify <code>wheel</code> and/or <code>setuptools</code> explicitly in the <code>host</code>
section of your recipe if you need them. At first, <code>pip</code> for versions of Python before 3.13 will be unaffected.
However, after the completion of the Python 3.13 migration, we will remove these dependencies from <code>pip</code> for all
versions of Python. Follow GitHub issue <a href="https://github.com/conda-forge/conda-forge.github.io/issues/2252" target="_blank" rel="noopener noreferrer" class="">#2252</a>
for more information and updates.</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sunsetting PyPy support]]></title>
        <id>https://conda-forge.org/news/2024/08/14/sunsetting-pypy/</id>
        <link href="https://conda-forge.org/news/2024/08/14/sunsetting-pypy/"/>
        <updated>2024-08-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR: We are planning to remove PyPy from conda-forge feedstock recipes in a]]></summary>
        <content type="html"><![CDATA[<p>TL;DR: We are planning to remove PyPy from conda-forge feedstock recipes in a
few weeks (and thus to stop building new releases of packages for PyPy), unless
there is substantial enough interest to justify the continued maintenance effort.</p>
<p>Conda-forge introduced <a href="https://conda-forge.org/blog/2020/03/10/pypy/" target="_blank" rel="noopener noreferrer" class="">support</a>
for <a href="https://pypy.org/index.html" target="_blank" rel="noopener noreferrer" class="">PyPy</a> as an alternative implementation of
Python about 4.5 years ago.</p>
<p>In that time we have worked hard together with developers from PyPy to provide
easily installable pre-compiled builds of the most common libraries also for PyPy.</p>
<p>As a very positive side-effect, the infrastructure of conda-forge is now fully
equipped to deal with alternative implementations of the Python interpreter,
which will continue to be useful (for example for supporting the experimental
free-threading builds of CPython 3.13).</p>
<p>However, due to a lack of resources ‚Äì both in terms of expertise for PyPy, as
well as available time of those who can help ‚Äì the conda-forge builds for PyPy
packages have been in minimal maintenance for a while (for example, we never
migrated for PyPy 3.10, nor did PyPy participate in the NumPy 2.0 migration).</p>
<p>As a consequence, we unfortunately need to announce that we are sunsetting
support for PyPy. This means we will stop the long-running migrator to add
PyPy to new feedstocks, and we will begin removing PyPy builds from feedstocks
at the latest on October 1st, or when we begin migrating for CPython 3.13
(whichever comes earlier).</p>
<p>If you are depending on PyPy builds in some way, please let us know about the
specifics of your situation in the discussion issue for the
<a href="https://github.com/conda-forge/conda-forge.github.io/issues/2255" target="_blank" rel="noopener noreferrer" class="">RFC</a>.</p>
<p>There is a possibility that we receive the necessary support from the PyPy
developers, if it turns out that enough people depend on PyPy support in
conda-forge for it to be worth their time. As such, please speak up if you
fall into this category!</p>
<p>Finally, no matter the outcome, we want to congratulate the PyPy developers
for their important contributions and exploration of what is possible in the
Python ecosystem! üëè</p>
<p>It has been a pleasure to collaborate!</p>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sunsetting Mambaforge]]></title>
        <id>https://conda-forge.org/news/2024/07/29/sunsetting-mambaforge/</id>
        <link href="https://conda-forge.org/news/2024/07/29/sunsetting-mambaforge/"/>
        <updated>2024-07-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[With the Miniforge 23.3.1 release,]]></summary>
        <content type="html"><![CDATA[<p>With the <a href="https://github.com/conda-forge/miniforge/releases/tag/23.3.1-0" target="_blank" rel="noopener noreferrer" class="">Miniforge 23.3.1 release</a>,
the Miniforge and Mambaforge installers became essentially <strong>identical</strong>. The only difference
between the two was their name and, subsequently, the default installation directory.</p>
<p>Maintaining both installers adds unnecessary burden to our support infrastructure and creates
confusion among end users. As a result, last year we decided to <em>discourage</em> the usage of
Mambaforge in favor of Miniforge.</p>
<p>A year later, we would like to formally sunset Mambaforge. All users are encouraged to switch to
Miniforge. Remember they contain the same packages, so the only difference is the default
installation directory.</p>
<p>The deprecation process will roughly follow this calendar:</p>
<ul>
<li class="">August 2024: The Mambaforge installers will warn about the deprecation and make the user wait 30
seconds before continuing.</li>
<li class="">October 2024: The Mambaforge installers will refuse to install during several pre-specified date ranges (i.e., <a href="https://en.wikipedia.org/wiki/Brownout_(software_engineering)" target="_blank" rel="noopener noreferrer" class="">"brownouts"</a>) in order to encourage users to switch to Miniforge. These dates are
<ul>
<li class="">Every two weeks in October 2024 (2024-10-01, 2024-10-15)</li>
<li class="">Every ten days in November 2024 (2024-11-01, 2024-11-10, 2024-11-20, 2024-11-30)</li>
<li class="">Every five days in December 2024 (2024-12-05, 2024-12-10, 2024-12-15, 2024-12-20, 2024-12-25, 2024-12-30, 2024-12-31)</li>
</ul>
</li>
<li class="">January 2025: The Mambaforge installers will stop being built and distributed.</li>
</ul>]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[NumPy 2 Migration]]></title>
        <id>https://conda-forge.org/news/2024/05/15/numpy-2-migration/</id>
        <link href="https://conda-forge.org/news/2024/05/15/numpy-2-migration/"/>
        <updated>2024-05-15T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[In preparation for the NumPy 2 release, we have begun a migration of all]]></summary>
        <content type="html"><![CDATA[<p>In preparation for the NumPy 2 release, we have begun a migration of all
conda-forge packages that depend on <code>numpy</code> during the build (so in
<code>requirements/host</code>). The migration status is tracked
<a class="" href="https://conda-forge.org/status/migration/?name=numpy2">on our status page</a>.
Particular package's status can be found there. Specific recommendations for
maintainers updating packages can be found in the migration PRs themselves.
Please reach out through the usual channels if you have any questions.</p>]]></content>
    </entry>
</feed>